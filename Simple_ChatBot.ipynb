{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4AjSd9BJoSLgW1v6kQ7Dr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaheenPafiast/-simple-ai-python-projects/blob/main/Simple_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Adiprogrammer7/intent_based_chatbot.git\n",
        "%cd intent_based_chatbot\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlWzX20P1ZnY",
        "outputId": "f2eb9e48-dca8-49a4-8a3a-a877c175c4f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'intent_based_chatbot'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 76 (delta 34), reused 67 (delta 29), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (76/76), 208.34 KiB | 1.67 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "/content/intent_based_chatbot/intent_based_chatbot/intent_based_chatbot/intent_based_chatbot\n",
            "chatbot_model.tflearn.data-00000-of-00001  README.md\n",
            "chatbot_model.tflearn.index\t\t   requirements.txt\n",
            "chatbot_model.tflearn.meta\t\t   saved_variables.pickle\n",
            "checkpoint\t\t\t\t   training_chatbot.ipynb\n",
            "intents.json\t\t\t\t   utils.py\n",
            "main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install tensorflow nltk\n"
      ],
      "metadata": {
        "id": "cIzFPLQs1Zki"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Core resources needed for word_tokenize + lemmatizer\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")     # newer NLTK sometimes needs this\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")       # WordNet helper\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqff9VpQ1ZiD",
        "outputId": "e73f4131-8611-4fe2-d59a-21889b0d2dc9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random, pickle, numpy as np\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# ---- Load intents.json safely ----\n",
        "with open(\"intents.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "if \"intents\" not in intents or not isinstance(intents[\"intents\"], list):\n",
        "    raise ValueError(\"âŒ intents.json must contain a top-level key 'intents' as a list.\")\n",
        "\n",
        "words, labels, docs_x, docs_y = [], [], [], []\n",
        "\n",
        "ignore = set(list(\"?!.,;:()'\\\"-\"))\n",
        "\n",
        "for intent in intents[\"intents\"]:\n",
        "    tag = intent.get(\"tag\", None)\n",
        "    patterns = intent.get(\"patterns\", [])\n",
        "    responses = intent.get(\"responses\", [])\n",
        "\n",
        "    # minimal validation\n",
        "    if not tag or not isinstance(patterns, list) or not isinstance(responses, list):\n",
        "        continue\n",
        "\n",
        "    if tag not in labels:\n",
        "        labels.append(tag)\n",
        "\n",
        "    for pattern in patterns:\n",
        "        if not isinstance(pattern, str) or not pattern.strip():\n",
        "            continue\n",
        "\n",
        "        tokens = nltk.word_tokenize(pattern)\n",
        "        tokens = [t.lower() for t in tokens if t not in ignore]\n",
        "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "        if len(tokens) == 0:\n",
        "            continue\n",
        "\n",
        "        words.extend(tokens)\n",
        "        docs_x.append(tokens)\n",
        "        docs_y.append(tag)\n",
        "\n",
        "# ---- Hard stop if no data ----\n",
        "if len(docs_x) == 0:\n",
        "    raise ValueError(\"âŒ No training patterns found. Check intents.json -> intents -> patterns.\")\n",
        "\n",
        "words = sorted(list(set(words)))\n",
        "labels = sorted(list(set(labels)))\n",
        "\n",
        "print(\"âœ… Samples:\", len(docs_x))\n",
        "print(\"âœ… Vocab size:\", len(words))\n",
        "print(\"âœ… Labels:\", labels)\n",
        "\n",
        "label_to_idx = {lab: i for i, lab in enumerate(labels)}\n",
        "\n",
        "# ---- Build BoW training vectors ----\n",
        "X, y = [], []\n",
        "for tokens, tag in zip(docs_x, docs_y):\n",
        "    bow = [1.0 if w in tokens else 0.0 for w in words]\n",
        "    X.append(bow)\n",
        "\n",
        "    out = [0.0] * len(labels)\n",
        "    out[label_to_idx[tag]] = 1.0\n",
        "    y.append(out)\n",
        "\n",
        "X = np.array(X, dtype=np.float32)\n",
        "y = np.array(y, dtype=np.float32)\n",
        "\n",
        "# ---- TF2 Model ----\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(len(words),)),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(len(labels), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(X, y, epochs=200, batch_size=8, verbose=0)\n",
        "final_acc = float(history.history[\"accuracy\"][-1])\n",
        "print(f\"âœ… Training complete. Final accuracy: {final_acc:.3f}\")\n",
        "\n",
        "# ---- Save everything ----\n",
        "model.save(\"chatbot_tf2.keras\")\n",
        "with open(\"tf2_assets.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\"words\": words, \"labels\": labels, \"intents\": intents}, f)\n",
        "\n",
        "print(\"âœ… Saved: chatbot_tf2.keras + tf2_assets.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkZ1nuxW1Zfl",
        "outputId": "f84594f4-8112-4596-9700-22a6ec4dc8bb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Samples: 34\n",
            "âœ… Vocab size: 59\n",
            "âœ… Labels: ['address', 'booking', 'goodbye', 'greeting', 'multiple_rooms', 'name', 'one_room', 'thanks']\n",
            "âœ… Training complete. Final accuracy: 1.000\n",
            "âœ… Saved: chatbot_tf2.keras + tf2_assets.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle, numpy as np, random\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "ignore = set(list(\"?!.,;:()'\\\"-\"))\n",
        "\n",
        "with open(\"tf2_assets.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "words = data[\"words\"]\n",
        "labels = data[\"labels\"]\n",
        "intents = data[\"intents\"]\n",
        "model = tf.keras.models.load_model(\"chatbot_tf2.keras\")\n",
        "\n",
        "def clean_sentence(text: str):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [t.lower() for t in tokens if t not in ignore]\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "def make_bow(tokens):\n",
        "    vec = np.zeros(len(words), dtype=np.float32)\n",
        "    token_set = set(tokens)\n",
        "    for i, w in enumerate(words):\n",
        "        if w in token_set:\n",
        "            vec[i] = 1.0\n",
        "    return vec.reshape(1, -1)\n",
        "\n",
        "def find_responses(tag):\n",
        "    for intent in intents[\"intents\"]:\n",
        "        if intent.get(\"tag\") == tag:\n",
        "            res = intent.get(\"responses\", [])\n",
        "            if isinstance(res, list) and len(res) > 0:\n",
        "                return res\n",
        "    return [\"I don't have a response for that yet.\"]\n",
        "\n",
        "def chatbot_reply(user_text, threshold=0.55):\n",
        "    tokens = clean_sentence(user_text)\n",
        "    if len(tokens) == 0:\n",
        "        return \"Please type something ðŸ™‚\", None, 0.0\n",
        "\n",
        "    probs = model.predict(make_bow(tokens), verbose=0)[0]\n",
        "    idx = int(np.argmax(probs))\n",
        "    conf = float(probs[idx])\n",
        "    tag = labels[idx]\n",
        "\n",
        "    if conf < threshold:\n",
        "        return \"Sorry, I didn't understand. Try asking in a different way.\", None, conf\n",
        "\n",
        "    return random.choice(find_responses(tag)), tag, conf\n",
        "\n",
        "print(\"âœ… Chatbot running (type: quit to stop)\\n\")\n",
        "\n",
        "while True:\n",
        "    user = input(\"You: \").strip()\n",
        "    if user.lower() in {\"quit\", \"exit\"}:\n",
        "        print(\"Bot: Bye!\")\n",
        "        break\n",
        "\n",
        "    ans, tag, conf = chatbot_reply(user, threshold=0.55)\n",
        "    if tag is None:\n",
        "        print(f\"Bot: {ans}  (conf={conf:.2f})\")\n",
        "    else:\n",
        "        print(f\"Bot ({tag}, conf={conf:.2f}): {ans}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okcUyXO71Zcv",
        "outputId": "531d263b-1469-412a-8158-c382cc8307b4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Chatbot running (type: quit to stop)\n",
            "\n",
            "You: hello\n",
            "Bot (greeting, conf=1.00): Hello! Good to see you!\n",
            "You: how are you\n",
            "Bot (greeting, conf=1.00): Hey genius, what's up?\n",
            "You: good\n",
            "Bot (goodbye, conf=0.94): Goodbye! Have a nice day.\n",
            "You: Tomorro I have paper of NLP\n",
            "Bot: Sorry, I didn't understand. Try asking in a different way.  (conf=0.49)\n",
            "You: i have paper tomorro\n",
            "Bot (goodbye, conf=0.83): See you soon pal...\n",
            "You: why you talk with me like this\n",
            "Bot (thanks, conf=0.60): Happy to help!\n",
            "You: ok \n",
            "Bot: Sorry, I didn't understand. Try asking in a different way.  (conf=0.43)\n",
            "You: tomorrow is my Final Exam\n",
            "Bot (address, conf=0.59): Our hotel is located at 'Malibu Point 10880, 90265' :)\n",
            "You: Quit to stop\n",
            "Bot: Sorry, I didn't understand. Try asking in a different way.  (conf=0.47)\n",
            "You: Quit\n",
            "Bot: Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-vvOPYUe1nQp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}